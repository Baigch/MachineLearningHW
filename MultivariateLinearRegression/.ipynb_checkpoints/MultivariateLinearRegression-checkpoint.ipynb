{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Multivariate Linear Regression\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "\n",
    "对于所给的数据集，我们使用两个参数来进行训练，$x_1$ 和 $x_2$ 分别对应房子的面积和距离双鸭山职业技术学院的距离，可得出方程:\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2$$ \n",
    "\n",
    "为计算逻辑的准确和方便引入 $x_0 = 1$ ，转化为:$$h_\\theta(x) = \\theta^TX$$\n",
    "\n",
    "数据加载是逐行读取，用空格切分，存储为$np.array$ 格式；为保证梯度下降算法更快地收敛，对数据进行缩放处理，此处我们使用 $sklearn.preprocessing.MinMaxScaler$ 类对数据归一化，将数据映射到（0，1）的范围；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "# 数据加载\n",
    "def load_exdata(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split(' ')\n",
    "            current = [float(item) for item in line]\n",
    "            data.append(current)\n",
    "    return data\n",
    "# 加载训练集，存储为array\n",
    "data = load_exdata('dataForTraining.txt')\n",
    "data = np.array(data, np.float)\n",
    "x = data[:, (0, 1)].reshape((-1, 2))\n",
    "y = data[:, 2].reshape((-1, 1))\n",
    "m = y.shape[0]\n",
    "# 加载测试集，存储为array\n",
    "data1 = load_exdata('dataForTesting.txt')\n",
    "data1 = np.array(data1, np.float)\n",
    "x1 = data1[:, (0, 1)].reshape((-1, 2))\n",
    "y1 = data1[:, 2].reshape((-1, 1))\n",
    "m1 = y1.shape[0]\n",
    "# 特征缩放到0-1\n",
    "min_x = preprocessing.MinMaxScaler()\n",
    "min_y = preprocessing.MinMaxScaler()\n",
    "min_x1 = preprocessing.MinMaxScaler()\n",
    "min_y1 = preprocessing.MinMaxScaler()\n",
    "X_min = min_x.fit_transform(x)\n",
    "Y_min = min_y.fit_transform(y)\n",
    "X_min1 = min_x1.fit_transform(x1)\n",
    "Y_min1 = min_y1.fit_transform(y1)\n",
    "# 引入x0 = 1\n",
    "X = np.hstack([X_min, np.ones((X_min.shape[0], 1))])\n",
    "X1 = np.hstack([X_min1, np.ones((X_min1.shape[0], 1))])\n",
    "#print(X,X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数的计算通过计算损失平方和均值得到：\n",
    "\n",
    "$$J(\\theta_0, \\theta_1…\\theta_n) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算误差\n",
    "def computeCost(X, y, theta):\n",
    "    m = y.shape[0]\n",
    "    J = (np.sum((X.dot(theta) - y)**2)) / (2*m)\n",
    "    return J\n",
    "# theta = np.zeros((3, 1)) # 初始化theta\n",
    "# print(computeCost(X,Y_min,theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 $\\theta$ 的求解通过如下方程得到：\n",
    "\n",
    "$$\\theta_j = \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})\\cdot x_j^{(i)})$$\n",
    "\n",
    "为加快计算速度，可通过转化计算过程为向量化计算来实现，转化后损失函数可写为：\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m}(X\\theta- \\vec y)^T(X\\theta- \\vec y)$$\n",
    "\n",
    "则相应的 $\\theta$ 的迭代为：\n",
    "\n",
    "$$\\theta = \\theta -(\\frac{\\alpha}{m})*(X.T.dot(X.dot(\\theta)-y))$$\n",
    "\n",
    "将 $\\alpha $ 值设为0.00015，迭代1500000次，进行梯度下降模拟，实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降实现，X、y为训练集数据，X1、y1为测试集数据\n",
    "# 学习率alpha = 0.00015，迭代次数num_iters = 1500000\n",
    "def gradientDescent(X, y, X1, y1, theta, alpha, num_iters):\n",
    "    m = y.shape[0]\n",
    "    # 存储历史误差\n",
    "    J_history = np.zeros((num_iters, 1)) # 初始化训练集历史误差\n",
    "    J_history1 = np.zeros((num_iters,1)) # 初始化测试集历史误差\n",
    "    for iter in range(num_iters):\n",
    "        gradient = X.T.dot(X.dot(theta)-y)\n",
    "        theta = theta - (alpha / m) * gradient\n",
    "        J_history[iter] = computeCost(X, y, theta)\n",
    "        # 使用训练集得来的theta，带入测试集数据\n",
    "        J_history1[iter] = computeCost(X1, y1, theta)\n",
    "    return J_history, J_history1, theta\n",
    "iterations = 1500000  # 迭代次数\n",
    "alpha = 0.00015  # 学习率\n",
    "J_history, J_history1, theta = gradientDescent(X, Y_min, X1, Y_min1, theta, alpha, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "收集100000、200000、300000...1500000次迭代的数据:\n",
    "\n",
    "| num_iters | J_history      | J_history1 |\n",
    "| :-------- | -------------- | ---------- |\n",
    "| 0         | 0.09566953     | 0.25976532 |\n",
    "| 100000    | 0.01069353     | 0.06505139 |\n",
    "| 200000    | 0.00436441     | 0.06446628 |\n",
    "| 300000    | 0.00181569     | 0.06753992 |\n",
    "| 400000    | 0.00076038     | 0.07081555 |\n",
    "| 500000    | 0.00031982     | 0.07345639 |\n",
    "| 600000    | 0.00013547     | 0.07538376 |\n",
    "| 700000    | 5.82818471e-05 | 0.07672374 |\n",
    "| 800000    | 2.59533782e-05 | 0.07763037 |\n",
    "| 900000    | 1.24132207e-05 | 0.07823392 |\n",
    "| 1000000   | 6.74209782e-06 | 0.07863165 |\n",
    "| 1100000   | 4.36680957e-06 | 0.07889209 |\n",
    "| 1200000   | 3.37194461e-06 | 0.07906191 |\n",
    "| 1300000   | 2.95525555e-06 | 0.07917236 |\n",
    "| 1400000   | 2.78072955e-06 | 0.07924407 |\n",
    "| 1500000   | 2.70763156e-06 | 0.07929058 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "并绘制图像：\n",
    "\n",
    "![](images/00015.png)\n",
    "\n",
    "根据表格数据以及图像的直观反应，发现训练集误差的收敛值要小于测试集误差的收敛值，且测试集误差的收敛趋势是先降低又上升；\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像的绘制\n",
    "plt.plot(J_history, label=\"DataForTraining\")\n",
    "plt.plot(J_history1, '--', label=\"DataForTesting\")\n",
    "plt.legend(handles=[], labels=[], loc='center right')\n",
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "plt.ylabel('lost')\n",
    "plt.xlabel('iter count')\n",
    "plt.title('convergence graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "此时改变学习率 $ alpha = 0.0002$，再进行同样的运算，得到如下数据，下面的数据记录了迭代次数为0，100000，200000……1499999次时的历史误差，前两列为上一练习中学习率为0.00015时的历史误差数据，后两列为学习率为0.0002时的历史误差数据，发现学习率较大时，收敛速度更快，但在图像的反应上并不是非常明显；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1500000\n",
    "alpha1 = 0.0002  \n",
    "theta_1 = np.zeros((3, 1))\n",
    "J_history_1, J_history1_1, theta_1 = gradientDescent(X, Y_min, X1, Y_min1, theta_1, alpha1, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[训练集]            [测试集]       [训练集]            [测试集]\n",
    "[ 0.09566953]      [ 0.25976532] [ 0.09566032]      [ 0.25974846]\n",
    "[ 0.01069353]      [ 0.06505139] [ 0.0079024]       [ 0.06398865]\n",
    "[ 0.00436441]      [ 0.06446628] [ 0.00242968]      [ 0.06640763]\n",
    "[ 0.00181569]      [ 0.06753992] [ 0.00076037]      [ 0.07081557]\n",
    "[ 0.00076038]      [ 0.07081555] [ 0.00023994]      [ 0.07417359]\n",
    "[ 0.00031982]      [ 0.07345639] [  7.70031182e-05] [ 0.07633258]\n",
    "[ 0.00013547]      [ 0.07538376] [  2.59531930e-05] [ 0.07763038]\n",
    "[  5.82818471e-05] [ 0.07672374] [  9.95599124e-06] [ 0.07838546]\n",
    "[  2.59533782e-05] [ 0.07763037] [  4.94290782e-06] [ 0.07881728]\n",
    "[  1.24132207e-05] [ 0.07823392] [  3.37193710e-06] [ 0.07906192]\n",
    "[  6.74209782e-06] [ 0.07863165] [  2.87963502e-06] [ 0.07919979]\n",
    "[  4.36680957e-06] [ 0.07889209] [  2.72536010e-06] [ 0.07927726]\n",
    "[  3.37194461e-06] [ 0.07906191] [  2.67701427e-06] [ 0.07932072]\n",
    "[  2.95525555e-06] [ 0.07917236] [  2.66186392e-06] [ 0.07934507]\n",
    "[  2.78072955e-06] [ 0.07924407] [  2.65711619e-06] [ 0.07935872]\n",
    "[  2.70763156e-06] [ 0.07929058] [  2.65494933e-06] [ 0.07937608]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制图像,并与EX1所得图像对比：\n",
    "\n",
    "![](images/EX2-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试再次更改学习率 $alpha = 0.002$ ，进行同样的运算，记录数据如下（前两列alpha = 0.00015，后两列alpha= 0.002），下面的数据记录了迭代次数为0，100000，200000……1499999次时的历史误差，对比发现此时收敛速度比之前更快，对比更明显；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[训练集]            [测试集]       [训练集]            [测试集]\n",
    "[ 0.09566953]      [ 0.25976532] [  0.09532924]     [ 0.25914214]\n",
    "[ 0.01069353]      [ 0.06505139] [  2.87954351e-06] [ 0.07919982]\n",
    "[ 0.00436441]      [ 0.06446628] [  2.65495138e-06] [ 0.07937555]\n",
    "[ 0.00181569]      [ 0.06753992] [  2.65494933e-06] [ 0.07937608]\n",
    "[ 0.00076038]      [ 0.07081555] [  2.65494933e-06] [ 0.07937608]\n",
    "[ 0.00031982]      [ 0.07345639] [  2.65494933e-06] [ 0.07937608]\n",
    "[ 0.00013547]      [ 0.07538376] [  2.65494933e-06] [ 0.07937608]\n",
    "[  5.82818471e-05] [ 0.07672374] [  2.65494933e-06] [ 0.07937608]\n",
    "[  2.59533782e-05] [ 0.07763037] [  2.65494933e-06] [ 0.07937608]\n",
    "[  1.24132207e-05] [ 0.07823392] [  2.65494933e-06] [ 0.07937608]\n",
    "[  6.74209782e-06] [ 0.07863165] [  2.65494933e-06] [ 0.07937608]\n",
    "[  4.36680957e-06] [ 0.07889209] [  2.65494933e-06] [ 0.07937608]\n",
    "[  3.37194461e-06] [ 0.07906191] [  2.65494933e-06] [ 0.07937608]\n",
    "[  2.95525555e-06] [ 0.07917236] [  2.65494933e-06] [ 0.07937608]\n",
    "[  2.78072955e-06] [ 0.07924407] [  2.65494933e-06] [ 0.07937608]\n",
    "[  2.70763156e-06] [ 0.07929058] [  2.65494933e-06] [ 0.07937608]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将此时得到的图像再与EX1中所得对比, 变化更加明显：\n",
    "\n",
    "![](images/EX2-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "对于样本量过大的数据集，采用随机梯度下降的方法能提高运算速度，这种方法每次迭代只使用一组样本数据进行 $\\theta$ 值的更新，即每次随机出一组数据参与运算，而不用像**Ex.1**和**EX.2**那样，每次迭代时所有样本均参与一次计算;现在使用随机梯度下降的方式，学习率迭代次数不变，进行模拟，具体实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  theat 下降方向\n",
    "def dj_sgd(theat, X, y):\n",
    "    return X.T.dot(X.dot(theat)-y)\n",
    "\n",
    "# 随机梯度下降实现，X、y为训练集数据，X1、y1为测试集数据\n",
    "# 学习率alpha = 0.00015，迭代次数num_iters = 1500000\n",
    "def gradientDescent(X, y, X1, y1, theta, alpha, num_iters):\n",
    "    m = y.shape[0]\n",
    "    # 存储历史误差\n",
    "    J_history = np.zeros((num_iters, 1)) # 初始化训练集历史误差\n",
    "    J_history1 = np.zeros((num_iters,1)) # 初始化测试集历史误差\n",
    "    for iter in range(num_iters):\n",
    "      \trand_i = np.random.randint(len(X))\n",
    "        gradient = dj_sgd(theta, np.array([X[rand_i]]), np.array([y[rand_i]]))\n",
    "        theta = theta - alpha * gradient \n",
    "        # 由于参与运算的只有一组样本，故此处不用再除以样本数量m\n",
    "        J_history[iter] = computeCost(X, y, theta)\n",
    "        # 使用训练集得来的theta，带入测试集数据\n",
    "        J_history1[iter] = computeCost(X1, y1, theta)\n",
    "    return J_history, J_history1, theta\n",
    "J_history, J_history1, theta = gradientDescent(X, Y_min, X1, Y_min1, theta, alpha, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "收集数据如下，对比**EX.1**中的梯度下降方法，随机梯度下降在提高了运算速度的情况下依然能够保证收敛趋势；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[训练集]            [测试集]\n",
    "[ 0.09565896]      [ 0.25974751]\n",
    "[ 0.01068054]      [ 0.06461656]\n",
    "[ 0.00435615]      [ 0.06420935]\n",
    "[ 0.0018196]       [ 0.06781842]\n",
    "[ 0.00075958]      [ 0.07108264]\n",
    "[ 0.0003192]       [ 0.07340283]\n",
    "[ 0.00013626]      [ 0.07534867]\n",
    "[  5.84691014e-05] [ 0.07675526]\n",
    "[  2.59411901e-05] [ 0.07761141]\n",
    "[  1.23841278e-05] [ 0.07825227]\n",
    "[  6.73296593e-06] [ 0.07862582]\n",
    "[  4.35263948e-06] [ 0.07888495]\n",
    "[  3.36720027e-06] [ 0.07904713]\n",
    "[  2.95648267e-06] [ 0.07918798]\n",
    "[  2.78203450e-06] [ 0.07923039]\n",
    "[  2.70766048e-06] [ 0.07928182]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据下面的图像可以看出，误差在下降并收敛时（迭代次数在50000-600000之间），数据一直有小的波动，可以体现随机梯度下降时的过程；\n",
    "\n",
    "![](images/EX3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比在**EX.1** 得到的误差图，整体趋势上变化不大."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "编辑元数据",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
